<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/mimikyu_sprite-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/mimikyu_sprite-16.png">
  <link rel="mask-icon" href="/images/mimikyu1.png" color="#222">
  <meta name="google-site-verification" content="google-site-verification=ck4DTybsJf_iilhTQ_VvtDffi3o9LMPMY1YCnCcrexc">
  <meta name="baidu-site-verification" content="code-g0DzkvvtMn">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"willkyu.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.9.0","exturl":true,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":true,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="是什么（如定义、有哪些实例或者公式等等） 优缺点 应用场景 知识点实例之间的对比（如，relu和tanh有什么联系、区别）">
<meta property="og:type" content="article">
<meta property="og:title" content="基础知识01">
<meta property="og:url" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/index.html">
<meta property="og:site_name" content="丘丘Blog">
<meta property="og:description" content="是什么（如定义、有哪些实例或者公式等等） 优缺点 应用场景 知识点实例之间的对比（如，relu和tanh有什么联系、区别）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/third-order-tensor.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/three-fitting-curve.jpg">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/different-local-minimum.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/gradient-descent.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/Adagrad.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/RMSProp.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/RMSProp-Nesterov.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/Adam.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/P-R-Curve.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/ROC-AUC.png">
<meta property="og:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/KS.jpg">
<meta property="article:published_time" content="2022-07-03T02:33:40.000Z">
<meta property="article:modified_time" content="2022-07-06T08:58:15.065Z">
<meta property="article:author" content="willkyu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/pics01/third-order-tensor.png">


<link rel="canonical" href="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/","path":"Learning/Notes/2022Summer/the-Basics01/","title":"基础知识01"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>基础知识01 | 丘丘Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="丘丘Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">丘丘Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">willkyu's blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-text">张量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E8%8C%83%E6%95%B0"><span class="nav-text">张量的范数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%86%85%E7%A7%AF"><span class="nav-text">张量的内积</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-text">参数初始化策略</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%B8%B8%E6%95%B0%E8%BF%87%E5%A4%A7%E6%88%96%E8%BF%87%E5%B0%8F"><span class="nav-text">初始化为常数、过大或过小</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%98%AF%E5%A5%BD%E7%9A%84"><span class="nav-text">什么样的初始化是好的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-text">Xavier初始化策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kaiming%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-text">Kaiming初始化策略</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%8C%83%E6%95%B0%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">参数范数与正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-text">范数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E6%95%B0%E5%80%BC%E5%81%87%E8%AE%BE%E5%88%86%E6%9E%90"><span class="nav-text">简单数值假设分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-text">梯度爆炸与梯度消失</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-text">梯度爆炸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-text">梯度消失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0"><span class="nav-text">产生原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-text">解决方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B9%B6%E5%BE%AE%E8%B0%83"><span class="nav-text">预训练并微调</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E5%89%AA%E5%88%87%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">梯度剪切与正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8reluleakreluelu%E7%AD%89%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">使用relu、leakrelu、elu等激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E8%A7%84%E8%8C%83%E5%8C%96"><span class="nav-text">批规范化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84"><span class="nav-text">使用残差结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C"><span class="nav-text">LSTM（长短期记忆网络）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%AE%97%E6%B3%95"><span class="nav-text">自适应学习率算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#adagrad"><span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rmsprop"><span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam"><span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-text">评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87"><span class="nav-text">分类指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87%E7%B2%BE%E5%BA%A6accuracy"><span class="nav-text">准确率（精度，Accuracy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E5%87%86%E7%8E%87%E7%B2%BE%E7%A1%AE%E7%8E%87precision"><span class="nav-text">查准率（精确率，Precision）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E5%85%A8%E7%8E%87%E5%8F%AC%E5%9B%9E%E7%8E%87recall"><span class="nav-text">查全率（召回率，Recall）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#p-r%E6%9B%B2%E7%BA%BF%E5%B9%B3%E8%A1%A1%E7%82%B9%E5%92%8Cf1%E8%A1%A1%E9%87%8F"><span class="nav-text">P-R曲线、平衡点和F1衡量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#p-r%E6%9B%B2%E7%BA%BF"><span class="nav-text">P-R曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B3%E8%A1%A1%E7%82%B9break-even-pointbep"><span class="nav-text">平衡点（Break-Even-Point，BEP）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#f1%E5%BA%A6%E9%87%8F"><span class="nav-text">F1度量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E6%83%85%E5%86%B5"><span class="nav-text">多分类情况</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#roc%E4%B8%8Eauc"><span class="nav-text">ROC与AUC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ks%E5%9B%BEkolomogorov-smirnov-chart"><span class="nav-text">KS图（Kolomogorov Smirnov
chart）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%8C%87%E6%A0%87"><span class="nav-text">回归指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AEmean-squared-errormse%E4%B8%8E%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AEroot-mean-squared-errorrmse"><span class="nav-text">均方误差（Mean
Squared Error，MSE）与均方根误差（Root Mean Squared Error，RMSE）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AEmean-absolute-errormae"><span class="nav-text">平均绝对误差（Mean Absolute
Error，MAE）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0-r%E6%96%B9r-squarded"><span class="nav-text">决定系数 R方（R-squarded）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="nav-text">归一化的好处</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">归一化的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E6%A0%87%E5%87%86%E5%8C%96min-max-normalization"><span class="nav-text">最大最小标准化（Min-Max
Normalization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#z-score%E6%A0%87%E5%87%86%E5%8C%96"><span class="nav-text">z-score标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">神经网络归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E5%87%BD%E6%95%B0%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">对数函数归一化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">反正切函数归一化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#l2%E8%8C%83%E6%95%B0%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">L2范数归一化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E4%BD%BF%E7%94%A8%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">什么时候使用归一化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dropout"><span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-text">反向传播算法（公式推导）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-text">过拟合、欠拟合</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="willkyu"
      src="/images/avatar_.png">
  <p class="site-author-name" itemprop="name">willkyu</p>
  <div class="site-description" itemprop="description">爱你所爱 求你所求 得你所得</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button site-overview-item animated">
    <button class="js-gitter-toggle-chat-button"><i class="fa fa-comment"></i>
      Chat Room
    </button>
  </div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dpbGxreXU=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;willkyu"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOjQ5NjM3MzE1OEBxcS5jb20=" title="E-Mail → mailto:496373158@qq.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS93aWxsa3l1" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;willkyu"><i class="fab fa-twitter fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly93b2thbm4uZ2l0aHViLmlv" title="https:&#x2F;&#x2F;wokann.github.io">WoKann Blog</span>
        </li>
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cDovL21pZmFuLXV4LmdpdGh1Yi5pbw==" title="http:&#x2F;&#x2F;mifan-ux.github.io">MiFan Blog</span>
        </li>
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly93dWx1bjAxMDIuZ2l0aHViLmlv" title="https:&#x2F;&#x2F;wulun0102.github.io">WuLun Blog</span>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dpbGxreXU=" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar_.png">
      <meta itemprop="name" content="willkyu">
      <meta itemprop="description" content="爱你所爱 求你所求 得你所得">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丘丘Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基础知识01
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-03 10:33:40" itemprop="dateCreated datePublished" datetime="2022-07-03T10:33:40+08:00">2022-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-06 16:58:15" itemprop="dateModified" datetime="2022-07-06T16:58:15+08:00">2022-07-06</time>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>是什么（如定义、有哪些实例或者公式等等）</p>
<p>优缺点</p>
<p>应用场景</p>
<p>知识点实例之间的对比（如，relu和tanh有什么联系、区别）</p>
<span id="more"></span>
<h1 id="张量">张量</h1>
<p>张量（tensor）是一个<strong>多维数组</strong>，张量的<strong>阶数（order）</strong>也称为维度（dimensions）。</p>
<p>一阶张量是一个<strong>矢量</strong>，二阶张量是一个<strong>矩阵</strong>，三阶或更高阶的张量叫做<strong>高阶张量</strong>。</p>
<!--
![三阶张量](/images/body/the-Basics01/third-order-tensor.png "A third-order tensor")
-->
<figure>
<img data-src="pics01/third-order-tensor.png" title="A third-order tensor"
alt="三阶张量" />
<figcaption aria-hidden="true">三阶张量</figcaption>
</figure>
<h2 id="张量的范数">张量的范数</h2>
<p>张量 <span class="math inline">\(\mathscr{X} \in \mathbb{R}^{I_1
\times I_2 \times \cdots \times I_N}\)</span>
的范数（norm）是其<strong>所有元素平方和的平方根</strong>，即</p>
<p><span class="math display">\[
\Vert \mathscr{X} \Vert =
\sqrt{\sum_{i_1=1}^{I_1}{\sum_{i_2=1}^{I_2}{\cdots
\sum_{i_N=1}^{I_N}{x_{i_1i_2 \cdots i_N}^2}}}}
\]</span></p>
<h2 id="张量的内积">张量的内积</h2>
<p>两个相同大小的张量 <span class="math inline">\(\mathscr{X} ,
\mathscr{Y} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times
I_N}\)</span> 的内积（inner product）为</p>
<p><span class="math display">\[
\langle \mathscr{X} , \mathscr{Y} \rangle =
\sum_{i_1=1}^{I_1}{\sum_{i_2=1}^{I_2} {\cdots
\sum_{i_N=1}^{I_N}{x_{i_1i_2 \cdots i_N} y_{i_1i_2 \cdots i_N}}}}
\]</span></p>
<p>且有 <span class="math inline">\(\langle \mathscr{X} , \mathscr{X}
\rangle = \Vert \mathscr{X} \Vert^2\)</span></p>
<h1 id="参数初始化策略">参数初始化策略</h1>
<p>深度学习中，神经网络的参数初始化策略对模型的收敛速度及性能有至关重要的影响。在神经网络中，随着层数的增多，在梯度下降的过程中极易出现<a
href="#梯度爆炸梯度消失">梯度消失或梯度爆炸</a>的问题，一个好的参数初始化对于处理这两个问题有着很大帮助。</p>
<h2 id="初始化为常数过大或过小">初始化为常数、过大或过小</h2>
<p>这些初始化策略都是不可取的，常数初始化会使得每层所有神经元相等，效果等效于一个神经元，极大限制了神经网络的学习能力。而参数过大过小会导致模型出现梯度爆炸或梯度消失的问题。</p>
<p>具体推导过程可见：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMzgwNjQxODg=">https://zhuanlan.zhihu.com/p/138064188<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="什么样的初始化是好的">什么样的初始化是好的？</h2>
<ul>
<li>因为对参数 <span class="math inline">\(w\)</span>
的大小和正负缺乏先验知识，<span class="math inline">\(w\)</span>
应为随机数，且期望 <span class="math inline">\(E(w) = 0\)</span> 。</li>
<li>为了防止梯度爆炸和梯度消失，权重不宜过大或过小，要对权重的方差 <span
class="math inline">\(Var(w)\)</span> 有所控制。</li>
<li>由于 <span class="math inline">\(dW_{l} = \frac{1}{m}dZ_{l}\cdot
A_{l-1}^T\)</span>，<span class="math inline">\(dW_{l}\)</span> 还与
<span class="math inline">\(A_{l-1}^T\)</span>
有关，所以我们希望不同激活层输出的方差相同，即 <span
class="math inline">\(Var(a_l) = Var(a_{l-1})\)</span>
，也就意味着不同激活层输入的方差相同，即 <span
class="math inline">\(Var(z_l) = Var(z_{l-1})\)</span> 。</li>
<li>权重的数值范围应考虑到前向与后向两个过程，不能过大或过小。</li>
</ul>
<h2 id="xavier初始化策略">Xavier初始化策略</h2>
<p>论文地址：<a
href="https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding
the difficulty of training deep feedforward neural networks</a></p>
<p><strong>核心思想：正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度的方差保持不变</strong></p>
<p>Xavier初始化将每层权重设置在有界的随即均匀分布中选择的值</p>
<p><span class="math display">\[
\pm \frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}
\]</span></p>
<p>其正态分布形式为</p>
<p><span class="math display">\[
(u, \sigma^2) = (0, \frac{2}{n_{i-1}+n_{i}})
\]</span></p>
<p>Xavier初始化策略对<strong>使用关于零对称且在[-1,
1]内有输出的激活函数（如softsign和tanh）</strong>效果较好，而如果使用ReLU激活函数则会产生梯度消失。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def Xavier(m, h):</span><br><span class="line">    return torch.Tensor(m, h).uniform_(-1, 1) * math.sqrt(6. / (m + h))</span><br><span class="line"></span><br><span class="line">x = torch.randn(512)</span><br><span class="line">for i in range(100):</span><br><span class="line">    a = Xavier(512, 512)</span><br><span class="line">    x = torch.tanh(a @ x)</span><br><span class="line">print(x.mean(), x.std())</span><br><span class="line"></span><br><span class="line">x = torch.randn(512)</span><br><span class="line">for i in range(100):</span><br><span class="line">    a = Xavier(512, 512)</span><br><span class="line">    x = torch.relu(a @ x)</span><br><span class="line">print(x.mean(), x.std())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor(0.0072) tensor(0.0723)</span><br><span class="line">tensor(5.5722e-16) tensor(8.1033e-16)</span><br></pre></td></tr></table></figure>
<h2 id="kaiming初始化策略">Kaiming初始化策略</h2>
<p>论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDIuMDE4NTI=">Delving Deep
into Rectifiers: Surpassing Human-Level Performance on ImageNet
Classification<i class="fa fa-external-link-alt"></i></span></p>
<p><strong>核心思想：正向传播时，状态值的方差保持不变；反向传播时，关于激活值的梯度的方差保持不变</strong></p>
<p>Kaiming初始化（也称he初始化）是针对Relu激活函数的初始化方法，作者证明了如果采用一下输入权重初始化策略，深层网络会更早收敛：</p>
<ul>
<li>使用适合给定图层的权重矩阵创建张量，并使用从标准正态分布中随机选择的数字填充它。</li>
<li>将每个随机选择的数字乘以<span
class="math inline">\(\frac{\sqrt{2}}{\sqrt{n}}\)</span>，其中n是从前一层输出到指定层的连接数（fan-in）</li>
<li>偏差张量初始化为零。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def Kaiming(m, h):</span><br><span class="line">    return torch.randn(m, h) * math.sqrt(2. / m)</span><br><span class="line"></span><br><span class="line">x = torch.randn(512)</span><br><span class="line">for i in range(100):</span><br><span class="line">    a = Kaiming(512, 512)</span><br><span class="line">    x = torch.relu(a @ x)</span><br><span class="line">print(x.mean(), x.std())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor(0.7144) tensor(1.0703)</span><br></pre></td></tr></table></figure>
<h1 id="参数范数与正则化">参数范数与正则化</h1>
<p>正则化（regularization）技术广泛应用在机器学习和深度学习算法中，其<strong>本质作用是防止过拟合、提高模型泛化能力</strong>。在早期的机器学习领域一般只是将范数惩罚叫做正则化技术，而在深度学习领域认为：能够显著减少方差，而不过度增加偏差的策略都可以认为是正则化技术，故推广的正则化技术还有：扩增样本集、早停止、Dropout、集成学习、多任务学习、对抗训练、参数共享等</p>
<h2 id="范数">范数</h2>
<ol type="1">
<li>P范数：</li>
</ol>
<p><span class="math display">\[
L_p = (\sum_{i = 1}^n\vert x_i\vert^p)^{\frac{1}{p}}
\]</span></p>
<ol start="2" type="1">
<li>L0范数：表示向量中非零元素的个数</li>
<li>L1范数：为向量元素绝对值之和</li>
</ol>
<p><span class="math display">\[
\Vert x\Vert _1 = \sum_{i = 1}^n\vert x_i\vert
\]</span></p>
<ol start="4" type="1">
<li>L2范数：向量元素的平方和再开方，也称欧几里得距离</li>
</ol>
<p><span class="math display">\[
\Vert x\Vert _2 = \sqrt{\sum_{i = 1}^nx_i^2}
\]</span></p>
<ol start="5" type="1">
<li><span class="math inline">\(\infty\)</span>
范数：即所有向量元素绝对值中的最大值</li>
</ol>
<p><span class="math display">\[
\Vert x\Vert _\infty = \max_i\vert x_i\vert
\]</span></p>
<ol start="6" type="1">
<li><span class="math inline">\(-\infty\)</span>
范数：即所有向量元素绝对值中的最小值</li>
</ol>
<p><span class="math display">\[
\Vert x\Vert _{-\infty} = \min_i\vert x_i\vert
\]</span></p>
<h2 id="简单数值假设分析">简单数值假设分析</h2>
<!--
![不同参数下的曲线拟合结果(左欠拟合，右过拟合)](/images/body/the-Basics01/three-fitting-curve.jpg "Three fitting curve")
-->
<figure>
<img data-src="pics01/three-fitting-curve.jpg" title="Three fitting curve"
alt="不同参数下的曲线拟合结果(左欠拟合，右过拟合)" />
<figcaption
aria-hidden="true">不同参数下的曲线拟合结果(左欠拟合，右过拟合)</figcaption>
</figure>
<p>对于右边的拟合曲线，有</p>
<p><span class="math display">\[
h_\theta(x) = \theta _0 + \theta _1x_1 + \theta _2x_2^2 + \theta _3x_3^3
+ \theta _4x_4^4
\]</span></p>
<p>由于 <span class="math inline">\(\theta _3\)</span> 和 <span
class="math inline">\(\theta _4\)</span>
对应了高阶，导致拟合曲线是4阶曲线，出现了过拟合。正则化的目的为适当缩减
<span class="math inline">\(\theta _3\)</span> 和 <span
class="math inline">\(\theta _4\)</span>
的值，例如都为0.0001，则上述曲线本质上等价于</p>
<p><span class="math display">\[
h_\theta(x) = \theta _0 + \theta _1x_1 + \theta _2x_2^2
\]</span></p>
<p>也就是变成了中间的刚好合适的拟合曲线。对 <span
class="math inline">\(\theta _3\)</span> 和 <span
class="math inline">\(\theta _4\)</span>
增加L2正则项后的代价函数表达式为</p>
<p><span class="math display">\[
J(\theta) = \min_\theta \frac{1}{n} \sum_{i = 1}^n{((h_\theta(x^i) -
y^i)^2 + 1000\theta_3^2 + 1000\theta_4^2)}
\]</span></p>
<p>从上式可以看出， <span class="math inline">\(\theta _3\)</span> 和
<span class="math inline">\(\theta _4\)</span>
均大于0，其乘上了1000，要是 <span
class="math inline">\(J(\theta)\)</span> 最小，则会迫使模型学习到的
<span class="math inline">\(\theta _3\)</span> 和 <span
class="math inline">\(\theta _4\)</span> 会非常小，因为只有在 <span
class="math inline">\(\theta _3\)</span> 和 <span
class="math inline">\(\theta _4\)</span>
非常小的情况下整个代价函数值才会取的较小值。在实际开发中，是对所有参数进行正则化，为了使代价函数尽可能的小，所有的参数
<span class="math inline">\(\theta\)</span> 的值（不包括 <span
class="math inline">\(\theta_0\)</span>
）都会在一定程度上减小，但是减少程度会不一样，从而实现了权重衰减、简化模型复杂度的作用。</p>
<h1 id="梯度下降法">梯度下降法</h1>
<p>优化算法的功能是通过改善训练方法，来最小化（或最大化）损失函数 <span
class="math inline">\(J(\theta)\)</span>。</p>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合 <span
class="math inline">\((\theta _0, \theta _1, \theta _2, \dots, \theta
_n)\)</span>
，计算损失函数，然后我们寻找下一个能让损失函数值下降最多的参数组合。我们持续这么做直到到一个局部最小值（local
minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global
minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>
<!--
![不同初始参数组合找到不同的极小值](/images/body/the-Basics01/different-local-minimum.png "Different local minimum")
-->
<figure>
<img data-src="pics01/different-local-minimum.png"
title="Different local minimum"
alt="不同初始参数组合找到不同的极小值" />
<figcaption
aria-hidden="true">不同初始参数组合找到不同的极小值</figcaption>
</figure>
<p>梯度下降算法如下：</p>
<p><span class="math display">\[
\theta _j := \theta _j - \alpha\frac{\partial}{\partial\theta
_j}J(\theta)
\]</span></p>
<p>其中 <span class="math inline">\(\alpha\)</span> 是学习率（learning
rate），它决定了我们沿着该方向迈出的步子有多大。<span
class="math inline">\(\alpha\)</span>
太大或太小都不好，太小会导致收敛速度很慢，而太大可能会越过最低点，甚至无法收敛。</p>
<!--
![梯度下降法](/images/body/the-Basics01/different-local-minimum.png "Gradient descent")
-->
<figure>
<img data-src="pics01/gradient-descent.png" title="Gradient descent"
alt="梯度下降法" />
<figcaption aria-hidden="true">梯度下降法</figcaption>
</figure>
<p>传统的批量梯度下降法计算整个数据集梯度，但只进行一次更新，这在处理大型数据集时速度很慢且难以控制，甚至导致内存溢出。</p>
<p>权重的更新速度由学习率 <span class="math inline">\(\alpha\)</span>
决定，并且可以在凸面误差曲线中收敛到全局最优值，在非凸曲面中可能趋于局部最优值。</p>
<p>此外，标准形式的批量梯度下降法在训练大型数据集时存在冗杂的权重更新。</p>
<h1 id="梯度爆炸与梯度消失">梯度爆炸与梯度消失</h1>
<h2 id="梯度爆炸">梯度爆炸</h2>
<p>误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。
在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致
NaN 值。 网络层之间的梯度（值大于
1.0）重复相乘导致的指数级增长会产生梯度爆炸。在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的NaN权重值。</p>
<h2 id="梯度消失">梯度消失</h2>
<p>在某些情况下，梯度会变得非常小，有效地阻止了权重值的变化。在最坏的情况下，这可能会完全停止神经网络的进一步训练。例如，传统的激活函数(如双曲正切函数)具有范围(0,1)内的梯度，反向传播通过链式法则计算梯度。这样做的效果是，用这些小数字的n乘以n来计算n层网络中“前端”层的梯度，这意味着梯度(误差信号)随n呈指数递减，而前端层的训练非常缓慢。</p>
<h2 id="产生原因">产生原因</h2>
<p>主要是采用了不合适的损失函数,
如果我们使用标准化初始w，那么各个层次的相乘都是0-1之间的小数，而激活函数f的导数也是0-1之间的数，其连乘后，结果会变的很小，导致梯度消失。若我们初始化的w是很大的数，w大到乘以激活函数的导数都大于1，那么连乘后，可能会导致求导的结果很大，形成梯度爆炸。</p>
<h2 id="解决方法">解决方法</h2>
<h3 id="预训练并微调">预训练并微调</h3>
<p>基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。</p>
<h3 id="梯度剪切与正则化">梯度剪切与正则化</h3>
<p>梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p>
<p>另外一种解决梯度爆炸的手段是采用权重正则化（weithts
regularization）比较常见的是L1正则化，和L2正则化，在各个深度框架中都有相应的API可以使用正则化。</p>
<h3
id="使用reluleakreluelu等激活函数">使用relu、leakrelu、elu等激活函数</h3>
<p>Relu:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。详见<a
href="#激活函数">激活函数</a>。</p>
<h3 id="批规范化">批规范化</h3>
<p>Batchnorm是深度学习发展以来提出的最重要的成果之一，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch
normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。</p>
<h3 id="使用残差结构">使用残差结构</h3>
<p>自从残差提出后，几乎所有的深度网络都离不开残差的身影，相比较之前的几层，几十层的深度网络，在残差网络面前都不值一提，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分。</p>
<h3 id="lstm长短期记忆网络">LSTM（长短期记忆网络）</h3>
<p>在RNN网络结构中，由于使用Logistic或者Tanh函数，所以很容易导致梯度消失的问题，即在相隔很远的时刻时，前者对后者的影响几乎不存在了，LSTM的机制正是为了解决这种长期依赖问题。</p>
<h1 id="自适应学习率算法">自适应学习率算法</h1>
<p>学习率对模型的性能有显著的影响，是难以设置的超参数之一。损失通常高度敏感于参数空间中的某些方向，而不敏感于其他。动量算法可以在一定程度缓解这些问题，但这样做的代价是引入了另一个超参数。</p>
<p>Delta-bar-delta 算法 (Jacobs, 1988)
是一个早期的在训练时适应模型参数各自学习率的启发式方法。该方法基于一个很简单的想法，如果损失对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。如果对于该参数的偏导变化了符号，那么学习率应减小。当然，这种方法只能应用于全批量优化中。最近，提出了一些增量（或者基于小批量）的算法来自适应模型参数的学习率。</p>
<h2 id="adagrad">AdaGrad</h2>
<p>Adagrad方法是在每个时间步中，根据过往已计算的参数梯度，来为每个参数修改对应的学习率。能独立地适应所有模型参数的学习率，当参数损失偏导值比较大时，有一个较大的学习率；当参数的损失偏导值较小时，有一个较小的学习率。</p>
<p>Adagrad方法的主要好处是，不需要手工来调整学习率。大多数参数使用了默认值0.01，且保持不变。</p>
<p>Adagrad方法的主要缺点是，学习率总是在降低和衰减。</p>
<p>因为每个附加项都是正的，在分母中累积了多个平方梯度值，故累积的总和在训练期间保持增长。这反过来又导致学习率下降，变为很小数量级的数字，该模型完全停止学习，停止获取新的额外知识。</p>
<p>因为随着学习速度的越来越小，模型的学习能力迅速降低，而且收敛速度非常慢，需要很长的训练和学习，即学习速度降低。</p>
<!--
![Adagrad算法](/images/body/the-Basics01/Adagrad.png "Adagrad")
-->
<figure>
<img data-src="pics01/Adagrad.png" title="Adagrad" alt="Adagrad算法" />
<figcaption aria-hidden="true">Adagrad算法</figcaption>
</figure>
<h2 id="rmsprop">RMSProp</h2>
<p>RMSProp 算法 (Hinton, 2012) 修改 AdaGrad
以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。</p>
<p>AdaGrad
旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。
AdaGrad
根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。</p>
<p>RMSProp
使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的
AdaGrad 算法实例。相比于 AdaGrad，使用移动平均引入了一个新的超参数 <span
class="math inline">\(\rho\)</span>，用来控制移动平均的长度范围。经验上，RMSProp
已被证明是一种有效且实用的深度神经网络优化算法，是深度学习从业者经常采用的优化方法之一。</p>
<!--
![RMSProp算法](/images/body/the-Basics01/RMSProp.png "RMSProp")
-->
<figure>
<img data-src="pics01/RMSProp.png" title="RMSProp" alt="RMSProp算法" />
<figcaption aria-hidden="true">RMSProp算法</figcaption>
</figure>
<!--
![使用Nesterov动量的RMSProp算法](/images/body/the-Basics01/RMSProp-Nesterov.png "RMSProp Nesterov")
-->
<figure>
<img data-src="pics01/RMSProp-Nesterov.png" title="RMSProp Nesterov"
alt="使用Nesterov动量的RMSProp算法" />
<figcaption
aria-hidden="true">使用Nesterov动量的RMSProp算法</figcaption>
</figure>
<h2 id="adam">Adam</h2>
<p>Adam (Kingma and Ba, 2014)
是另一种学习率自适应的优化算法，它可以被看作结合 RMSProp
和具有一些重要区别的动量的变种。</p>
<p>首先，在 Adam
中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp
最直观的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。</p>
<p>其次， Adam
包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。
RMSProp 也采用了（非中心的）二阶矩估计，然而缺失了修正因子。因此，不像
Adam，RMSProp 二阶矩估计可能在训练初期有很高的偏置。</p>
<p>Adam
通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要自行从默认值修改。</p>
<!--
![Adam算法](/images/body/the-Basics01/Adam.png "Adam")
-->
<figure>
<img data-src="pics01/Adam.png" title="Adam" alt="Adam算法" />
<figcaption aria-hidden="true">Adam算法</figcaption>
</figure>
<h1 id="评估指标">评估指标</h1>
<h2 id="分类指标">分类指标</h2>
<p><strong>混淆矩阵（confusion
matrix）</strong>是一个评估分类问题常用的工具，对于 k
元分类，其实它就是一个 k*k
的表格，用来记录分类器的预测结果。对于常见的二分类，它的混淆矩阵是 2*2
的。</p>
<p>在二分类问题中，可以将样例根据其真实类别和预测类别的组合划分为：</p>
<ul>
<li>真正例（true positive）TP</li>
<li>假正例（false positive）FP</li>
<li>真反例（true negative）TN</li>
<li>假反例（false negative）FN</li>
</ul>
<p>显然 TP + FP + TN + FN = 样例总数。分类结果的混淆矩阵如下:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">真实情况</th>
<th style="text-align: center;">预测</th>
<th style="text-align: center;">结果</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;">正例</td>
<td style="text-align: center;">反例</td>
</tr>
<tr class="even">
<td style="text-align: center;">正例</td>
<td style="text-align: center;">TP(真正例)</td>
<td style="text-align: center;">FN(假反例)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">反例</td>
<td style="text-align: center;">FP(假正例)</td>
<td style="text-align: center;">TN(真反例)</td>
</tr>
</tbody>
</table>
<h3 id="准确率精度accuracy">准确率（精度，Accuracy）</h3>
<p>精度Accurac是指模型预测正确（包括真正例TP、真反例TN）的样本数与总体样本数的占比，即：</p>
<p><span class="math display">\[
Accuracy = \frac{Count(correct)}{Count(total)}
\]</span></p>
<p>在二分类问题中：</p>
<p><span class="math display">\[
Accuracy = \frac{TP + TN}{TP + FP + FN + TN}
\]</span></p>
<p>准确率是分类问题中最简单直观的指标，但是在实际中应用不多。原因是：当样本标签分布不均衡时，比如：正样本占比99%，只要模型把所有样本都预测为正样本，则准确率达到99%，但是实际上模型根本没有预测能力。</p>
<h3 id="查准率精确率precision">查准率（精确率，Precision）</h3>
<p>查准率是从预测正例的角度出发（分子分母都是关于Positive）：</p>
<p><span class="math display">\[
Precision = \frac{TP}{TP + FP}
\]</span></p>
<p>假设我们用模型预测了一批西瓜，预测100个瓜为好瓜，其中60个为好瓜，40个坏瓜，则查准率就是60/100=60%.</p>
<p>查准率评估了模型预测的正例中的精度--准确更重要。</p>
<p>假设我们提高阈值，使得预测为正例的数量变少，预测精度更高，那么查准率就变得更高，因此单一查准率指标依然没法对模型性能进行准确评估。因为模型遗漏了大量正例，使得大量正例被误判为负例。</p>
<h3 id="查全率召回率recall">查全率（召回率，Recall）</h3>
<p>召回率从实际正例的角度出发（真正例、假反例）</p>
<p><span class="math display">\[
Recall = \frac{TP}{TP + FN}
\]</span></p>
<p>假设80个好瓜，模型预测出60个好瓜，20个好瓜未被正确预测，则查全率就是60/(60+20)=75%。</p>
<p>召回率评估了针对正例样本，表示样本中的正例有多少被预测正确--少漏更重要，要全。</p>
<p>假设我们降低阈值，就会使得预测为正例的样本数量增多，但查准率会降低。</p>
<p>所以查准率和查全率的矛盾，跟阈值相关。</p>
<h3 id="p-r曲线平衡点和f1衡量">P-R曲线、平衡点和F1衡量</h3>
<h4 id="p-r曲线">P-R曲线</h4>
<p>在上面分析查准率P，查全率R的时候，我们得到了不同阈值下对P、R的影响。那么当我们预测100的样本后（假设样本预测结果以概率形式输出），我们对样本结果进行排序，排在最前面（概率最大）的模型认为“最可能是”正例的样本，排在最后的是模型认为”最不可能“是正例的样本。按此顺序设置不同的阈值（阈值可以是排序后的概率值，或者固定划分点），在不同的阈值下，计算出当前阈值下的查准率P和查全率R。以查准率为纵轴，查全率为横轴作图，就可以得到查准率-查全率曲线，简称P-R曲线，显示改曲线的图称为”P-R图“。</p>
<!--
![P-R曲线与平衡点示意图](/images/body/the-Basics01/P-R-Curve.png "P-R Curve")
-->
<figure>
<img data-src="pics01/P-R-Curve.png" title="P-R Curve"
alt="P-R曲线与平衡点示意图" />
<figcaption aria-hidden="true">P-R曲线与平衡点示意图</figcaption>
</figure>
<p>P-R图直观地显示出学习器在样本总体上的查全率，查准率。在进行比较时，<strong>若一个学习期的P-R曲线被另一个学习器完全”包住“，则可以断言后者的性能优于前者</strong>。例如图中，学习器A的性能优于学习器C；如果两个学习器的P-R曲线发生了交叉，例如图中的A和B，则难以一般性地断言两者优劣，只能在具体地查准率或者查全率条件下进行比较。</p>
<p>然而，在很多情况下，人们往往仍然希望把A和B比出个高低。这时，一个比较合理地判断依据是<strong>比较P-R曲线下面积的大小（Area
under curve
P-R，AUC-PR）</strong>，它在一定程度上表征了学习器在查准率和查全率上取得相对”双高“的比例。但这个值不太容易估算，因此，人们设计了一些综合考虑查准率，查全率的性能度量，比如BEP度量，F1度量。</p>
<h4 id="平衡点break-even-pointbep">平衡点（Break-Even-Point，BEP）</h4>
<p>平衡点（Break-Even-Point），它是”查准率=查全率“时的取值，例如图1中的学习器C的BEP是0.64，基于BEP的比较，可认为A优于B。</p>
<h4 id="f1度量">F1度量</h4>
<p>BEP过于简单，这个平衡点是建立在”查准率=查全率“的前提下，无法满足实际不同场景的应用。我们引入加权调和平均
<span class="math inline">\(F_\beta\)</span> ：</p>
<p><span class="math display">\[
\frac{1}{F_\beta} = \frac{1}{1 + \beta ^2}(\frac{1}{P} + \frac{\beta
^2}{R})
\]</span></p>
<p>当 <span class="math inline">\(\beta = 1\)</span> 时，有 <span
class="math inline">\(\frac{1}{F_1} = \frac{1}{2}(\frac{1}{P} +
\frac{1}{R})\)</span>，即</p>
<p><span class="math display">\[
F_1 = \frac{2 * P * R}{P + R}
\]</span></p>
<p>在一些应用中，对查准率和查全率的重视程度不同。例如在商品推荐中，为了尽可能少打扰用户，更希望推荐的内容确实是用户感兴趣的，此时查准率更重要；而在罪犯信息检索或者病人检查系统中，更希望尽可能少的漏判，此时查全率更重要。F1度量的一般形式是
<span
class="math inline">\(F_\beta\)</span>，能让我们自定义对查准率/查全率的不同偏好：</p>
<p><span class="math display">\[
F_\beta = \frac{(1 + \beta ^2) * P * R}{(\beta ^2 * P) + R}
\]</span></p>
<p>其中，<span class="math inline">\(\beta &gt; 0\)</span>
度量了查全率对查准率的相对重要性，<span class="math inline">\(\beta =
1\)</span> 时退化为标准F1，<span class="math inline">\(\beta &gt;
1\)</span> 时查全率有更大影响；<span class="math inline">\(\beta &lt;
1\)</span> 时，查准率有更大影响。</p>
<h4 id="多分类情况">多分类情况</h4>
<p>很多时候我们有多个二分类混淆矩阵，例如进行多次训练/测试，每次得到一个混淆矩阵；或是在多个数据集上进行训练/测试，希望估计算法的全局性能；或者是执行分类任务，每两两类别的组合都对应一个混淆矩阵；总之是在n个二分类混淆矩阵上综合考察查准率和查全率。</p>
<p>一种直接的做法是现在各个混淆矩阵上分别计算出查准率和查全率，记为(P1,R1)，(P2,R2),...(Pn,Rn)，在计算平均值，这样就得到“宏观查准率”(macro-P)，“宏观查全率”(macro-R)、“宏观F1”(macro-F1)：</p>
<p><span class="math display">\[
macroP = \frac{1}{n}\sum^n_{i = 1} P_i\\
macroR = \frac{1}{n}\sum^n_{i = 1} R_i\\
macroF1 = \frac{2 * macroP * macroR}{macroP + macroR}
\]</span></p>
<p>另一种方法可以将个混淆矩阵对应的元素进行平均，得到TP、FP、TN、FN的平均值，分别记为
<span class="math inline">\(\overline{TP}\)</span>、<span
class="math inline">\(\overline{FP}\)</span>、<span
class="math inline">\(\overline{FN}\)</span>、<span
class="math inline">\(\overline{TN}\)</span>，再基于这些平均值计算出“微观查准率”(micro-P)，“微观查全率”(micro-R)、“微观F1”(micro-F1)：</p>
<p><span class="math display">\[
microP = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}\\
microR = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}\\
microF1 = \frac{2 * microP * microR}{microP + microR}
\]</span></p>
<h3 id="roc与auc">ROC与AUC</h3>
<p>根据上面混淆矩阵的一系列指标计算，可以发现，将样本预测为正例或者负例是与一个分类阈值相关的。若大于阈值则分为正类，否则为反类。</p>
<p>实际上，根据概率预测结果，我们可以将测试样本进行排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面。这样，分类过程就相当于在这个排序中以某个“截断点”(cut
point)将样本分为两部分，前一部分作为正例，后一部分作为反例。</p>
<p>在不同的任务中，我们可以根据任务需求采用不同的阈值，若我们更重视查准率，则选择排序中靠前的位置，若更重视查全率，则可以选择靠后的位置进行截。因此，排序本身的好坏，提现了综合考虑学习器在不同任务下的“期望泛化性能”的好坏。ROC曲线则是从这个角度出发来研究学习器泛化性能的有效工具。</p>
<p>ROC全称是“受试者工作特性”(Receiver Operating
Characteristic)曲线。与上述介绍的P-R曲线相似，我们根据预测结果对样例进行排序，按此顺序逐个将样本预测结果作为阈值进行划分。之后计算两个指标：真正例率（True
Positive Rate，简称TPR），假正例率（False Positive
Rate，简称FPR），公式如下：</p>
<p><span class="math display">\[
TPR = \frac{TP}{TP + FN}\\
FPR = \frac{FP}{FP + TN}
\]</span></p>
<p>以TPR为纵轴，FPR为横轴，得到ROC曲线。在现实任务中通常只有有限个样本来绘制ROC图，此时无法产生图a中光滑的曲线，只能绘制出图b所示的近似曲线。绘制过程如介绍公式时所描述的一样，对样本预测结果进行排序，然后取第一个结果作为阈值，此时所有样本均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0)处标记一个点。随后将第二个样本预测结果作为阈值，得到坐标点，依次类推。</p>
<!--
![ROC曲线与AUC示意图](/images/body/the-Basics01/ROC-AUC.png "ROC AUC")
-->
<figure>
<img data-src="pics01/ROC-AUC.png" title="ROC AUC"
alt="ROC曲线与AUC示意图" />
<figcaption aria-hidden="true">ROC曲线与AUC示意图</figcaption>
</figure>
<p>当要比较两个学习器的性能优劣时，与P-R曲线相似，若一个学习器的ROC曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者；若两个学习器的ROC曲线发生交叉，则一般难以断言两者优劣。此时可以比较ROC曲线下的面积，即AUC(Area
under ROC Curve)。</p>
<p>很明显，AUC的结果不会超过 1，通常ROC曲线都在 y = x
这条直线上面，所以，AUC的值一般在 0.5 ~ 1 之间。</p>
<p>从定义可知，AUC可以通过对ROC曲线下各部分的面积求和而得。假定ROC曲线是由坐标{(x1,y1),(x2,y2)...}的点连接而成。AUC可估算为</p>
<p><span class="math display">\[
AUC = \frac{1}{2}\sum^{m - 1}_{i = 1}(x_{i + 1} - x_i)(y_i + y_{i + 1})
\]</span></p>
<h3 id="ks图kolomogorov-smirnov-chart">KS图（Kolomogorov Smirnov
chart）</h3>
<p>KS值是在模型中用于<strong>区分预测正负样本分隔程度的评价指标</strong>，一般应用于金融风控领域。如果将人口划分为两个独立的组，其中一组包含所有正例而另一组包含所有负例，则K-S值为100。</p>
<p>与ROC曲线相似，ROC是以FPR作为横坐标，TPR作为纵坐标，通过改变不同阈值，从而得到ROC曲线。而在KS曲线中，则是以阈值作为横坐标，以FPR和TPR作为纵坐标，ks曲线则为TPR-FPR，ks曲线的最大值通常为ks值。</p>
<p>为什么这样求KS值呢？我们知道，当阈值减小时，TPR和FPR会同时减小，当阈值增大时，TPR和FPR会同时增大。而在实际工程中，我们希望TPR更大一些，FPR更小一些，即TPR-FPR越大越好，即ks值越大越好。</p>
<p>可以理解TPR是收益，FPR是代价，ks值是收益最大。图中绿色线是TPR、蓝色线是FPR。</p>
<!--
![KS](/images/body/the-Basics01/KS.jpg "KS")
-->
<figure>
<img data-src="pics01/KS.jpg" title="KS" alt="KS" />
<figcaption aria-hidden="true">KS</figcaption>
</figure>
<p>在实际应用中，比如风控模型中，往往单一指标并不能真正比较两个学习器之间的优劣，比如A学习器KS：40，B学习器KS：39。并不能保证A学习器一定比B学习器表现好，就像考试100的同学一定比99分的同学优秀一样。在实际中，通常结合单一指标（比如KS）和图表，综合判断模型在实际应用中，哪一个模型更加有优势。</p>
<h2 id="回归指标">回归指标</h2>
<h3
id="均方误差mean-squared-errormse与均方根误差root-mean-squared-errorrmse">均方误差（Mean
Squared Error，MSE）与均方根误差（Root Mean Squared Error，RMSE）</h3>
<p>MSE计算的是拟合数据和原始数据对应样本点的误差的平方和的均值，其值越小说明拟合效果越好。</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum^N_{i = 1}(y^2_i - \^y^2_i)
\]</span></p>
<p>由于MSE与我们的目标变量的量纲不一致，为了保证量纲一致性，我们需要对MSE进行开方，即均方根误差：</p>
<p><span class="math display">\[
RMSE = \sqrt{\frac{1}{N}\sum^N_{i = 1}(y^2_i - \^y^2_i)}
\]</span></p>
<p>以下是RMSE需要考虑的要点：</p>
<ul>
<li>“平方根”使该指标能够显示大的偏差。</li>
<li>此度量标准的“平方”特性有助于提供更强大的结果，从而防止取消正负误差值。换句话说，该度量恰当地显示了错误的合理幅度。</li>
<li>它避免使用绝对误差值，这在数学计算中是非常不希望的。</li>
<li>当我们有更多样本时，使用RMSE重建误差分布被认为更可靠。</li>
<li>RMSE受到异常值的影响很大。因此，请确保在使用此指标之前已从数据集中删除了异常值。</li>
<li>与平均绝对误差( mean absolute
error)相比，RMSE提供更高的权重并惩罚大的错误。</li>
</ul>
<h3 id="平均绝对误差mean-absolute-errormae">平均绝对误差（Mean Absolute
Error，MAE）</h3>
<p>和 MSE 一样，这种度量方法也是在不考虑方向的情况下衡量误差大小。但和
MSE 的不同之处在于，MAE
需要像线性规划这样更复杂的工具来计算梯度。此外，MAE
对异常值更加稳健，因为它不使用平方。</p>
<p><span class="math display">\[
MAE = \frac{1}{N}\sum^N_{i = 1}\vert y_i - \^y_i\vert
\]</span></p>
<h3 id="决定系数-r方r-squarded">决定系数 R方（R-squarded）</h3>
<p>判定系数，其含义是也是解释回归模型的方差得分，其值取值范围是[0,1]，越接近于1说明自变量越能解释因变量的方差变化，值越小则说明效果越差。又称为the
coefficient of
determination。判断的是预测模型和真实数据的拟合程度，最佳值为1，同时可为负值。如果结果是0，就说明我们的模型跟瞎猜差不多。如果结果是1。就说明我们模型无错误。如果结果是0-1之间的数，就是我们模型的好坏程度。如果结果是负数。说明我们的模型还不如瞎猜。</p>
<p>R方可以理解为因变量y中的变异性能能够被估计的多元回归方程解释的比例，它衡量各个自变量对因变量变动的解释程度，<strong>其取值在0与1之间，其值越接近1，则变量的解释程度就越高，其值越接近0，其解释程度就越弱</strong>。</p>
<p>一般来说，增加自变量的个数，回归平方和会增加，残差平方和会减少，所以R方会增大；反之，减少自变量的个数，回归平方和减少，残差平方和增加。为了消除自变量的数目的影响，引入了调整的R方。</p>
<p><span class="math display">\[
R^2 &amp; = 1- \frac{\sum^m_{i = 1}(f_i - y_i)^2}{\sum^m_{i =
1}(\overline{y_i} - y_i)^2}\\
&amp; = \frac{\frac{1}{m}\sum^m_{i = 1}(f_i -
y_i)^2}{\frac{1}{m}\sum^m_{i = 1}(\overline{y_i} - y_i)^2}\\
&amp; = 1 - \frac{MSE(f, y)}{Var(y)}
\]</span></p>
<h1 id="归一化">归一化</h1>
<p>不同评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，</p>
<p>为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。其中，最典型的就是数据的归一化处理。</p>
<p>简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。</p>
<blockquote>
<p>奇异样本数据是指相对于其他输入样本特别大或特别小的样本矢量（即特征向量）</p>
</blockquote>
<p>奇异样本数据的存在会引起训练时间增大，同时也可能导致无法收敛，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；反之，不存在奇异样本数据时，则可以不进行归一化。</p>
<h2 id="归一化的好处">归一化的好处</h2>
<ol type="1">
<li>归一化后加快了梯度下降求最优解的速度，也即加快训练网络的收敛性；</li>
<li>归一化有可能提高精度</li>
</ol>
<h2 id="归一化的方法">归一化的方法</h2>
<h3 id="最大最小标准化min-max-normalization">最大最小标准化（Min-Max
Normalization）</h3>
<p><span class="math display">\[
x&#39; = \frac{x - \min{(x)}}{\max{(x)} - \min{(x)}}
\]</span></p>
<ol type="1">
<li><p>线性函数将原始数据线性化的方法转换到[0 1]的范围,
计算结果为归一化后的数据，X为原始数据；</p></li>
<li><p>本归一化方法比较适用在数值比较集中的情况；</p></li>
<li><p>缺陷：如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min。</p></li>
</ol>
<p><strong>应用场景</strong>：在不涉及距离度量、协方差计算、数据不符合正态分布的时候，如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0
255]的范围。</p>
<h3 id="z-score标准化">z-score标准化</h3>
<p><span class="math display">\[
x^* = \frac{x - \mu}{\sigma}
\]</span></p>
<p>其中，<span class="math inline">\(\mu\)</span>、<span
class="math inline">\(\sigma\)</span> 分别为原始数据集的均值和方差。</p>
<ol type="1">
<li><p>将原始数据集归一化为均值为0、方差1的数据集。</p></li>
<li><p>该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕。</p></li>
</ol>
<p>应用场景：在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，Z-score
standardization表现更好。</p>
<h3 id="神经网络归一化">神经网络归一化</h3>
<p>本归一化方法经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。</p>
<p>该方法包括对数、正切等，需要根据数据分布的情况，决定非线性函数的曲线：</p>
<h4 id="对数函数归一化">对数函数归一化</h4>
<p><span class="math display">\[
y = \log_{10}(x)
\]</span></p>
<p>以10为底的对数转换函数，对应的归一化方法为：</p>
<p><span class="math display">\[
x&#39; = \frac{\log_{10}(x)}{\log_{10}(max)}
\]</span></p>
<p>其中max表示样本数据的最大值，并且所有样本数据均要大于等于1.</p>
<h4 id="反正切函数归一化">反正切函数归一化</h4>
<p><span class="math display">\[
x&#39; = \frac{2}{\pi}\arctan(x)
\]</span></p>
<p>使用这个方法需要注意的是如果想映射的区间为[0，1]，则数据都应该大于等于0，小于0的数据将被映射到[－1，0]区间上.</p>
<h3 id="l2范数归一化">L2范数归一化</h3>
<p>特征向量中每个元素均除以向量的L2范数：</p>
<p><span class="math display">\[
x&#39;_i = \frac{x_i}{norm(x)}
\]</span></p>
<h2 id="什么时候使用归一化">什么时候使用归一化</h2>
<ol type="1">
<li><p>如果对输出结果范围有要求，用归一化。</p></li>
<li><p>如果数据较为稳定，不存在极端的最大最小值，用归一化。</p></li>
<li><p>如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。</p></li>
</ol>
<blockquote>
<p><strong>归一化与标准化不同</strong></p>
</blockquote>
<h1 id="dropout">Dropout</h1>
<h1 id="激活函数">激活函数</h1>
<h1 id="损失函数">损失函数</h1>
<h1 id="反向传播算法公式推导">反向传播算法（公式推导）</h1>
<h1 id="过拟合欠拟合">过拟合、欠拟合</h1>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>给我买杯奶茶呗 ♪( ´▽｀)</div>
  <button>
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="willkyu WeChatpay">
        <span>WeChatpay</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="willkyu Alipay">
        <span>Alipay</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Written by:  </strong>willkyu
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/" title="基础知识01">http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> unless stating additionally.
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/Life/Zheng-with-his-five-dads/" rel="prev" title="Zheng. w/ his five DADs">
                  <i class="fa fa-chevron-left"></i> Zheng. w/ his five DADs
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-infinity"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">willkyu</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>
<script class="next-config" data-name="gitter" type="application/json">{"enable":true,"room":"willkyu-github-io/community"}</script>
<script src="/js/third-party/chat/gitter.js"></script>
<script src="/js/third-party/chat/sidecar.v1.js" async defer></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.2.0/dist/quicklink.umd.js" integrity="sha256-4kQf9z5ntdQrzsBC3YSHnEz02Z9C1UeW/E9OgnvlzSY=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"http://willkyu.github.io/Learning/Notes/2022Summer/the-Basics01/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"willkyu","repo":"Hexo-Gitalk","client_id":"4d181641d25dd938916f","client_secret":"225a2edcdc643bb02ca476ce92f4b386a924b99a","admin_user":"willkyu","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"891d7bbefa86e08fc32739543c4465ef"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>


  <!--动态线条背景-->
  <script type="text/javascript"  opacity='0.6' zIndex="-2" count="110"
  src="/js/DIY/canvas-nest.min.js">
  </script>

  <!-- 页面点击特效-->
  <script type="text/javascript" src="/js/DIY/click.js"></script>
</body>
</html>
